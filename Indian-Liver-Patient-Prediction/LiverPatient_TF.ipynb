{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LiverPatient_TF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKKznKAlwcUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the required libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler,Normalizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq3P87f_wjAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocess(object):\n",
        "  def __init__(self):\n",
        "    self.url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\"\n",
        "    self.cols = ['age','gender','TB','DB','alkphos','sgpt','sgot','TP','albumin','AG_ratio','Is_liverPatient']\n",
        "    self.dataset = None\n",
        "  \n",
        "  def load_data(self):\n",
        "    self.dataset = pd.read_csv(self.url,names=self.cols)\n",
        "    print('shape of df :',self.dataset.shape)\n",
        "    return self.dataset\n",
        "  \n",
        "  def preprocess(self,data):\n",
        "    #preprocessing the data\n",
        "    # 1. Removing the duplicate rows\n",
        "    self.dataset = data.drop_duplicates()\n",
        "    print('Shape after removing the duplicates :',self.dataset.shape)\n",
        "    # checking for the Null values, #display no of null values by column\n",
        "    print(self.dataset.isnull().sum())\n",
        "    #dropping the 4 rows here\n",
        "    self.dataset = self.dataset[pd.notnull(self.dataset['AG_ratio'])]\n",
        "    return None\n",
        "  \n",
        "  def encode_label(self):\n",
        "    #changing the gender attribute to categorical type\n",
        "    # 0 is for 'Female' and 1 for 'Male\n",
        "    self.dataset.gender = pd.factorize(self.dataset.gender)[0] + 0.0\n",
        "    #Encoding the label for proper implementation in network (1 neuron in the output layer)\n",
        "    # Label 1 means \"a liver patient\" so encoding as 1\n",
        "    # Label 2 means \"not a liver patient\" so encoding as 0\n",
        "    self.dataset.loc[(self.dataset.Is_liverPatient == 2), 'Is_liverPatient'] = 0\n",
        "    return None\n",
        "  \n",
        "  def scaleData(self):\n",
        "    #Scaling the attribute values to be between [0,1]\n",
        "    scaler = MinMaxScaler()\n",
        "    cols_to_scale = ['age', 'TB', 'DB', 'alkphos', 'sgpt', 'sgot', 'TP', 'albumin', 'AG_ratio']\n",
        "    Dataset = pd.DataFrame(data=self.dataset)\n",
        "    self.dataset[cols_to_scale] = scaler.fit_transform(Dataset[cols_to_scale])\n",
        "    return None\n",
        "  \n",
        "  def get_dataset(self):\n",
        "    return self.dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5mUpOyjwmNG",
        "colab_type": "code",
        "outputId": "19bc342c-517c-4c9d-f01c-2d08f31d8f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "prep = Preprocess()\n",
        "dataset = prep.load_data()\n",
        "prep.preprocess(dataset)\n",
        "prep.encode_label()\n",
        "prep.scaleData()\n",
        "dataset = prep.get_dataset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of df : (583, 11)\n",
            "Shape after removing the duplicates : (570, 11)\n",
            "age                0\n",
            "gender             0\n",
            "TB                 0\n",
            "DB                 0\n",
            "alkphos            0\n",
            "sgpt               0\n",
            "sgot               0\n",
            "TP                 0\n",
            "albumin            0\n",
            "AG_ratio           4\n",
            "Is_liverPatient    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wn3wgIJwo7S",
        "colab_type": "code",
        "outputId": "12b8e3c0-949f-4a8f-ccd4-74e54e5b0463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Splitting the dataset into train and test splits\n",
        "# Taking the label out of the original preprocessed dataset\n",
        "X = dataset[['age', 'gender', 'TB', 'DB', 'alkphos', 'sgpt', 'sgot', 'TP', 'albumin', 'AG_ratio']]\n",
        "y = dataset['Is_liverPatient']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.70)  #test_size = 1-0.7 = 0.3\n",
        "#Changing them into numpy array as the last step\n",
        "X_train = X_train.to_numpy().astype(np.float32)\n",
        "y_train = y_train.to_numpy().astype(np.int32)\n",
        "X_test = X_test.to_numpy().astype(np.float32)\n",
        "y_test = y_test.to_numpy().astype(np.int32)\n",
        "print('X_train shape :',X_train.shape)\n",
        "print('y_train shape :',y_train.shape)\n",
        "print('X_test shape :',X_test.shape)\n",
        "print('y_test shape :',y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape : (396, 10)\n",
            "y_train shape : (396,)\n",
            "X_test shape : (170, 10)\n",
            "y_test shape : (170,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihEw77e0ws9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main procedure call\n",
        "\n",
        "#creating the tensors out of the numpy arrays\n",
        "X_train = tf.Variable(X_train, dtype=tf.float32, trainable=False)\n",
        "y_train = tf.Variable(y_train, dtype=tf.int32, trainable=False)\n",
        "X_test = tf.Variable(X_test, dtype=tf.float32, trainable=False)\n",
        "y_test = tf.Variable(y_test, dtype=tf.int32, trainable=False)\n",
        "\n",
        "#intializing the network parameters\n",
        "input_dimension = X_train.shape[1]\n",
        "num_classes = 1\n",
        "#layer_activations = [\"Relu\", \"Relu\", \"Softmax\"]\n",
        "layer_activations = [\"Relu\", \"Softmax\"]\n",
        "nodes_in_layers = [5, num_classes]\n",
        "\n",
        "#intializing the hyperparameters\n",
        "batch_size = 50\n",
        "epochs = 10\n",
        "alpha = 0.1\n",
        "\n",
        "#creating the tensor datasets for incorporating the batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train)).shuffle(100).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdqnuck6w3MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = []\n",
        "biases = []\n",
        "trainable_vars = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJSnLvaKxHOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,nodes in enumerate(nodes_in_layers):\n",
        "  if i == 0:\n",
        "    w = tf.Variable(np.random.randn(*(input_dimension,nodes)), dtype=tf.float32,trainable=True)\n",
        "    b = tf.Variable(np.zeros(shape=(nodes,)),dtype=tf.float32, trainable=True)\n",
        "  else:\n",
        "    w = tf.Variable(np.random.randn(*(nodes_in_layers[i-1],nodes)),dtype=tf.float32 ,trainable=True)\n",
        "    b = tf.Variable(np.zeros(shape=(nodes,)),dtype=tf.float32, trainable=True)\n",
        "  weights.append(w)\n",
        "  biases.append(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRtoHdV00iup",
        "colab_type": "code",
        "outputId": "8f32ae11-d0ed-4387-9ad3-3539725f2222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for i in range(len(weights)):\n",
        "  print(weights[i].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5)\n",
            "(5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo4dhg3mxku1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qTb2jBax3UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdMk4O2AygQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X):\n",
        "  for i in range(len(weights)):\n",
        "    if layer_activations[i] == 'Relu':\n",
        "      X = tf.nn.relu(tf.add(tf.matmul(X,weights[i]),biases[i]))\n",
        "    elif layer_activations[i] == 'Sigmoid':\n",
        "      X = tf.nn.sigmoid(tf.add(tf.matmul(X,weights[i]),biases[i]))\n",
        "    elif layer_activations[i] == 'Softmax':\n",
        "      X = tf.nn.softmax(tf.add(tf.matmul(X,weights[i]),biases[i]))\n",
        "    else:\n",
        "     assert 2 == 4\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH7W2hSQygYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_trainableVars():\n",
        "  for i in range(len(weights)):\n",
        "    trainable_vars.append(weights[i])\n",
        "  for i in range(len(biases)):\n",
        "    trainable_vars.append(biases[i])\n",
        "  return trainable_vars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OCGe7f0Rgmq",
        "colab_type": "text"
      },
      "source": [
        "Implementing the training in multiple different ways:\n",
        "\n",
        "1. Gradient Descent (the simplest one)\n",
        "2. SGD (Stocastic Gradient Descent)\n",
        "   (using 2 approaches i.e a) apply_gradients b) minimise()\n",
        "3. Adam\n",
        "  (using 2 approaches i.e a) apply_gradients b) minimise()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsUUAkzTNIId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient Descent (the simplest one)\n",
        "@tf.function\n",
        "def train(X,y):\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    predictions = predict(X)\n",
        "    loss = loss_object(y,predictions)\n",
        "    for i in range(len(weights)):\n",
        "      dloss_dw,dloss_db = tape.gradient(loss,[weights[i],biases[i]])\n",
        "      assert dloss_dw != None or dloss_db != None\n",
        "      weights[i].assign_sub(alpha * dloss_dw)\n",
        "      biases[i].assign_sub(alpha * dloss_db)\n",
        "    train_loss(loss)\n",
        "    train_accuracy(y,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua4DjrYIyFOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SGD (Stocastic Gradient Descent) ==> a) apply_gradients\n",
        "@tf.function\n",
        "def train(X,y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = predict(X)\n",
        "    loss = loss_object(y,predictions)\n",
        "    trainable_vars = get_trainableVars()\n",
        "  gradients = tape.gradient(loss,trainable_vars)\n",
        "  optimizer.apply_gradients(zip(gradients,trainable_vars))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(y,predictions)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWU5FMi2WvQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss():\n",
        "  return loss_object(y_train,predict(X_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SJ6k_ZySLqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SGD (Stocastic Gradient Descent) ==> b) minimise()\n",
        "\n",
        "def train(X,y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = predict(X)\n",
        "    #loss = loss_object(y,predictions)\n",
        "    trainable_vars = get_trainableVars()\n",
        "  optimizer.minimize(loss,trainable_vars)    #here the loss parameter needs to be callable, therefore the loss() function is created. \n",
        "  train_loss(loss_object(y,predictions))\n",
        "  train_accuracy(y,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJyQXoNlznFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test(X,y):\n",
        "  predictions = predict(X)\n",
        "  t_loss = loss_object(y,predictions)\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(y,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8FdZ52iz5v4",
        "colab_type": "code",
        "outputId": "e6d413ef-d7bb-43d2-b418-8a8227727749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(tf.math.confusion_matrix(labels=y_train,predictions=predict(X_train)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  0 113]\n",
            " [  0 283]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UilrE783Qs-n",
        "colab_type": "code",
        "outputId": "41cc4e87-9451-4551-8166-ad48f4f00fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(tf.math.confusion_matrix(labels=y_test,predictions=predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  0  49]\n",
            " [  0 121]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrEhmyoE11Yy",
        "colab_type": "code",
        "outputId": "c77f6ca0-f914-4ea5-850a-95a50c3aa422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# starting the training\n",
        "for epoch in range(epochs):\n",
        "  #reset the metric in the beginning of each epoch \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  for x,label in train_ds:\n",
        "    train(x,label)\n",
        "  \n",
        "  for x,label in test_ds:\n",
        "    test(x,label)\n",
        "  \n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch + 1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result() * 100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result() * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.5981529951095581, Accuracy: 71.51087188720703, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 2, Loss: 0.5985878109931946, Accuracy: 71.46739959716797, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 3, Loss: 0.5981529951095581, Accuracy: 71.51087188720703, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 4, Loss: 0.5983704328536987, Accuracy: 71.4891357421875, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 5, Loss: 0.5972834229469299, Accuracy: 71.59782409667969, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 6, Loss: 0.5981530547142029, Accuracy: 71.51087188720703, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 7, Loss: 0.5977182388305664, Accuracy: 71.55435180664062, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 8, Loss: 0.5972834229469299, Accuracy: 71.59782409667969, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 9, Loss: 0.5990225672721863, Accuracy: 71.42391204833984, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 10, Loss: 0.5970661044120789, Accuracy: 71.61956787109375, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yke8cbnPRVZ",
        "colab_type": "code",
        "outputId": "75e0b944-31bf-4b58-d1f8-21c3e0faf6a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# starting the training\n",
        "for epoch in range(epochs):\n",
        "  #reset the metric in the beginning of each epoch \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  for x,label in train_ds:\n",
        "    train(x,label)\n",
        "  \n",
        "  for x,label in test_ds:\n",
        "    test(x,label)\n",
        "  \n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch + 1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result() * 100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result() * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.5979356169700623, Accuracy: 71.5326156616211, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 2, Loss: 0.5979356169700623, Accuracy: 71.53260040283203, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 3, Loss: 0.5983704328536987, Accuracy: 71.48912811279297, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 4, Loss: 0.5977182388305664, Accuracy: 71.55435180664062, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 5, Loss: 0.5985878109931946, Accuracy: 71.46739196777344, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 6, Loss: 0.5981529951095581, Accuracy: 71.51087188720703, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 7, Loss: 0.5983704328536987, Accuracy: 71.48912811279297, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 8, Loss: 0.5981529951095581, Accuracy: 71.51087188720703, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 9, Loss: 0.5983704328536987, Accuracy: 71.48912811279297, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n",
            "Epoch 10, Loss: 0.5992400050163269, Accuracy: 71.40216827392578, Test Loss: 0.6032617688179016, Test Accuracy: 71.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}