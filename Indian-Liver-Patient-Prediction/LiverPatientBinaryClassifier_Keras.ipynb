{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LiverPatientBinaryClassifier_Keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etnoaz406MBf",
        "colab_type": "text"
      },
      "source": [
        "**Indian Liver Patient Keras Sequential model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7DAkB_T6Ldt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential,layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler,Normalizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U32r_UEb7Y0C",
        "colab_type": "text"
      },
      "source": [
        "Data Preparation using Pandas and Sklearn libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDX9jITs7Wa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocess(object):\n",
        "  def __init__(self):\n",
        "    self.url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv\"\n",
        "    self.cols = ['age','gender','TB','DB','alkphos','sgpt','sgot','TP','albumin','AG_ratio','Is_liverPatient']\n",
        "    self.dataset = None\n",
        "  \n",
        "  def load_data(self):\n",
        "    self.dataset = pd.read_csv(self.url,names=self.cols)\n",
        "    print('shape of df :',self.dataset.shape)\n",
        "    return self.dataset\n",
        "  \n",
        "  def preprocess(self,data):\n",
        "    #preprocessing the data\n",
        "    # 1. Removing the duplicate rows\n",
        "    self.dataset = data.drop_duplicates()\n",
        "    print('Shape after removing the duplicates :',self.dataset.shape)\n",
        "    # checking for the Null values, #display no of null values by column\n",
        "    print(self.dataset.isnull().sum())\n",
        "    #dropping the 4 rows here\n",
        "    self.dataset = self.dataset[pd.notnull(self.dataset['AG_ratio'])]\n",
        "    return None\n",
        "  \n",
        "  def encode_label(self):\n",
        "    #changing the gender attribute to categorical type\n",
        "    # 0 is for 'Female' and 1 for 'Male\n",
        "    self.dataset.gender = pd.factorize(self.dataset.gender)[0] + 0.0\n",
        "    #Encoding the label for proper implementation in network (1 neuron in the output layer)\n",
        "    # Label 1 means \"a liver patient\" so encoding as 1\n",
        "    # Label 2 means \"not a liver patient\" so encoding as 0\n",
        "    self.dataset.loc[(self.dataset.Is_liverPatient == 2), 'Is_liverPatient'] = 0\n",
        "    return None\n",
        "  \n",
        "  def scaleData(self):\n",
        "    #Scaling the attribute values to be between [0,1]\n",
        "    scaler = MinMaxScaler()\n",
        "    cols_to_scale = ['age', 'TB', 'DB', 'alkphos', 'sgpt', 'sgot', 'TP', 'albumin', 'AG_ratio']\n",
        "    Dataset = pd.DataFrame(data=self.dataset)\n",
        "    self.dataset[cols_to_scale] = scaler.fit_transform(Dataset[cols_to_scale])\n",
        "    return None\n",
        "  \n",
        "  def get_dataset(self):\n",
        "    return self.dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMA5S1oq7iEj",
        "colab_type": "code",
        "outputId": "76d91b1e-80a7-442f-b1b0-e2ba5e7298a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "prep = Preprocess()\n",
        "dataset = prep.load_data()\n",
        "prep.preprocess(dataset)\n",
        "prep.encode_label()\n",
        "prep.scaleData()\n",
        "dataset = prep.get_dataset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of df : (583, 11)\n",
            "Shape after removing the duplicates : (570, 11)\n",
            "age                0\n",
            "gender             0\n",
            "TB                 0\n",
            "DB                 0\n",
            "alkphos            0\n",
            "sgpt               0\n",
            "sgot               0\n",
            "TP                 0\n",
            "albumin            0\n",
            "AG_ratio           4\n",
            "Is_liverPatient    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scR_c7517pZz",
        "colab_type": "code",
        "outputId": "6faabaff-fffe-426a-ffd3-70789ea97d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Splitting the dataset into train and test splits\n",
        "# Taking the label out of the original preprocessed dataset\n",
        "X = dataset[['age', 'gender', 'TB', 'DB', 'alkphos', 'sgpt', 'sgot', 'TP', 'albumin', 'AG_ratio']]\n",
        "y = dataset['Is_liverPatient']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.70)  #test_size = 1-0.7 = 0.3\n",
        "#Changing them into numpy array as the last step\n",
        "X_train = X_train.to_numpy().astype(np.float32)\n",
        "y_train = y_train.to_numpy().astype(np.int32)\n",
        "X_test = X_test.to_numpy().astype(np.float32)\n",
        "y_test = y_test.to_numpy().astype(np.int32)\n",
        "print('X_train shape :',X_train.shape)\n",
        "print('y_train shape :',y_train.shape)\n",
        "print('X_test shape :',X_test.shape)\n",
        "print('y_test shape :',y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape : (396, 10)\n",
            "y_train shape : (396,)\n",
            "X_test shape : (170, 10)\n",
            "y_test shape : (170,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxT159lJ7s4p",
        "colab_type": "text"
      },
      "source": [
        "Creating the keras Sequential Model and training it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kJuk2jj7y0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main procedure call\n",
        "\n",
        "#creating the tensors out of the numpy arrays\n",
        "'''\n",
        "X_train = tf.Variable(X_train, dtype=tf.float32, trainable=False)\n",
        "y_train = tf.Variable(y_train, dtype=tf.int32, trainable=False)\n",
        "X_test = tf.Variable(X_test, dtype=tf.float32, trainable=False)\n",
        "y_test = tf.Variable(y_test, dtype=tf.int32, trainable=False)\n",
        "'''\n",
        "#intializing the network parameters\n",
        "input_dimension = X_train.shape[1]\n",
        "num_classes = 1\n",
        "#layer_activations = [\"Relu\", \"Relu\", \"Softmax\"]\n",
        "#layer_activations = [\"Relu\", \"Softmax\"]\n",
        "#nodes_in_layers = [5, num_classes]\n",
        "\n",
        "#intializing the hyperparameters\n",
        "batch_size = 50\n",
        "epochs = 10\n",
        "alpha = 0.1\n",
        "\n",
        "#creating the tensor datasets for incorporating the batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train)).shuffle(100).batch(batch_size)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CP9I3n475lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Dense(units=10,activation='relu'))\n",
        "model.add(layers.Dense(units=1,activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUKHRv12-cGN",
        "colab_type": "code",
        "outputId": "7d39ae62-1679-4ced-e29b-a46cd9cd1d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model.build(input_shape=(1,10))\n",
        "model.call(tf.keras.Input(shape=(1,10)))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 1, 10)             110       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1, 1)              11        \n",
            "=================================================================\n",
            "Total params: 121\n",
            "Trainable params: 121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8_rElr8CDSW",
        "colab_type": "code",
        "outputId": "973d51b7-1c01-4dda-bbc8-83ea9fc394d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for layer in model.layers:\n",
        "  print('weight shape :',layer.get_weights()[0].shape)\n",
        "  print('bias shape :',layer.get_weights()[1].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight shape : (10, 5)\n",
            "bias shape : (5,)\n",
            "weight shape : (5, 1)\n",
            "bias shape : (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rf5IJ_r-jpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model compiled with sgd optimizer\n",
        "model.compile(optimizer='sgd',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgECxXGOD03N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model compiled with adam optimizer\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2JJNSLy_Dav",
        "colab_type": "code",
        "outputId": "fef8b71b-dfbb-4f25-e85c-835d521502ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "history = model.fit(X_train,y_train,epochs=epochs,validation_data=(X_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7020 - val_loss: 0.5721 - val_accuracy: 0.7412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAlScKqFCv9R",
        "colab_type": "code",
        "outputId": "e7cc2ff0-99aa-4d8d-ed78-9b6bfcc16057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#evaluating the model\n",
        "test_loss,test_acc = model.evaluate(X_test,y_test,verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6/6 - 0s - loss: 0.5721 - accuracy: 0.7412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH3RfU72DQci",
        "colab_type": "text"
      },
      "source": [
        "**Results:-**\n",
        "\n",
        "a) ***with SGD optimizer***\n",
        "\n",
        "The train loss = 0.6112, train acccuracy = 70.20%, val_loss = 0.5721 and val_accuracy = 74.12%\n",
        "\n",
        "b) ***with adam optimizer***\n",
        "\n"
      ]
    }
  ]
}